% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{gehring2017convolutional}
J.~Gehring, M.~Auli, D.~Grangier, D.~Yarats, and Y.~N. Dauphin, ``Convolutional
  sequence to sequence learning,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2017, pp. 1243--1252.

\bibitem{kalchbrenner-etal-2014-convolutional}
\BIBentryALTinterwordspacing
N.~Kalchbrenner, E.~Grefenstette, and P.~Blunsom, ``A convolutional neural
  network for modelling sentences,'' in \emph{Proceedings of the 52nd Annual
  Meeting of the Association for Computational Linguistics (Volume 1: Long
  Papers)}.\hskip 1em plus 0.5em minus 0.4em\relax Baltimore, Maryland:
  Association for Computational Linguistics, Jun. 2014, pp. 655--665. [Online].
  Available: \url{https://www.aclweb.org/anthology/P14-1062}
\BIBentrySTDinterwordspacing

\bibitem{kim-2014-convolutional}
\BIBentryALTinterwordspacing
Y.~Kim, ``Convolutional neural networks for sentence classification,'' in
  \emph{Proceedings of the 2014 Conference on Empirical Methods in Natural
  Language Processing ({EMNLP})}.\hskip 1em plus 0.5em minus 0.4em\relax Doha,
  Qatar: Association for Computational Linguistics, Oct. 2014, pp. 1746--1751.
  [Online]. Available: \url{https://www.aclweb.org/anthology/D14-1181}
\BIBentrySTDinterwordspacing

\bibitem{sutskever-2014-sequence}
I.~Sutskever, O.~Vinyals, and Q.~V. Le, ``Sequence to sequence learning with
  neural networks,'' in \emph{Proceedings of the 27th International Conference
  on Neural Information Processing Systems - Volume 2}, ser. NIPS'14.\hskip 1em
  plus 0.5em minus 0.4em\relax Cambridge, MA, USA: MIT Press, 2014, p.
  3104–3112.

\bibitem{liu-2016-recurrent}
P.~Liu, X.~Qiu, and X.~Huang, ``Recurrent neural network for text
  classification with multi-task learning,'' in \emph{Proceedings of the
  Twenty-Fifth International Joint Conference on Artificial Intelligence}, ser.
  IJCAI'16.\hskip 1em plus 0.5em minus 0.4em\relax AAAI Press, 2016, p.
  2873–2879.

\bibitem{socher-2013-recursivedeep}
R.~Socher, A.~Perelygin, J.~Y. Wu, J.~Chuang, C.~D. Manning, A.~Y. Ng, and
  C.~Potts, ``Recursive deep models for semantic compositionality over a
  sentiment treebank,'' in \emph{In Proceedings of EMNLP}, 2013, pp.
  1631--1642.

\bibitem{tai2015improved}
K.~S. Tai, R.~Socher, and C.~D. Manning, ``Improved semantic representations
  from tree-structured long short-term memory networks,'' in \emph{Proceedings
  of the 53rd Annual Meeting of the Association for Computational Linguistics
  and the 7th International Joint Conference on Natural Language Processing
  (Volume 1: Long Papers)}, 2015, pp. 1556--1566.

\bibitem{marcheggiani2018exploiting}
D.~Marcheggiani, J.~Bastings, and I.~Titov, ``Exploiting semantics in neural
  machine translation with graph convolutional networks,'' in \emph{Proceedings
  of the 2018 Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies, Volume 2 (Short
  Papers)}, 2018, pp. 486--492.

\bibitem{oquab2014learning}
M.~Oquab, L.~Bottou, I.~Laptev, and J.~Sivic, ``Learning and transferring
  mid-level image representations using convolutional neural networks,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2014, pp. 1717--1724.

\bibitem{thrun2004advances}
S.~Thrun, L.~K. Saul, and B.~Sch{\"o}lkopf, \emph{Advances in Neural
  Information Processing Systems 16: Proceedings of the 2003 Conference}.\hskip
  1em plus 0.5em minus 0.4em\relax MIT press, 2004, vol.~16.

\bibitem{minyoung2016ImageNet}
M.~Huh, P.~Agrawal, and A.~Efros, ``What makes imagenet good for transfer
  learning?'' 08 2016.

\bibitem{Deng2009ImageNet}
J.~{Deng}, W.~{Dong}, R.~{Socher}, L.~{Li}, {Kai Li}, and {Li Fei-Fei},
  ``Imagenet: A large-scale hierarchical image database,'' in \emph{2009 IEEE
  Conference on Computer Vision and Pattern Recognition}, 2009, pp. 248--255.

\bibitem{Russakovsky2015ImageNet}
\BIBentryALTinterwordspacing
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, A.~C. Berg, and L.~Fei-Fei, ``Imagenet
  large scale visual recognition challenge,'' \emph{Int. J. Comput. Vision},
  vol. 115, no.~3, p. 211–252, Dec. 2015. [Online]. Available:
  \url{https://doi.org/10.1007/s11263-015-0816-y}
\BIBentrySTDinterwordspacing

\bibitem{kentonbert}
J.~D. M.-W.~C. Kenton and L.~K. Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{Universal
  Language Model Fine-tuning for Text Classification}, p. 278.

\bibitem{dong2019unified}
L.~Dong, N.~Yang, W.~Wang, F.~Wei, X.~Liu, Y.~Wang, J.~Gao, M.~Zhou, and H.-W.
  Hon, ``Unified language model pre-training for natural language understanding
  and generation,'' 2019.

\bibitem{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov, ``Roberta: A robustly optimized bert
  pretraining approach,'' 2019.

\bibitem{hestnessdeep}
J.~Hestness, S.~Narang, N.~Ardalani, G.~Diamos, H.~Jun, H.~Kianinejad, M.~M.~A.
  Patwary, Y.~Yang, and Y.~Zhou, ``Deep learning scaling is predictable,
  empirically,'' 2017.

\bibitem{shazeer2017outrageously}
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~Le, G.~Hinton, and J.~Dean,
  ``Outrageously large neural networks: The sparsely-gated mixture-of-experts
  layer,'' 2017.

\bibitem{jozefowicz2016exploring}
R.~Jozefowicz, O.~Vinyals, M.~Schuster, N.~Shazeer, and Y.~Wu, ``Exploring the
  limits of language modeling,'' 2016.

\bibitem{mahajan2018exploring}
D.~Mahajan, R.~Girshick, V.~Ramanathan, K.~He, M.~Paluri, Y.~Li, A.~Bharambe,
  and L.~van~der Maaten, ``Exploring the limits of weakly supervised
  pretraining,'' in \emph{European Conference on Computer Vision}.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer, 2018, pp. 185--201.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever, ``Language
  models are unsupervised multitask learners,'' 2019.

\end{thebibliography}
